{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Change Belief Analysis\n",
    "## Predict an individual’s belief in climate change based on historical tweet data - for EDSA Online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes:\n",
    "\n",
    "\n",
    "\n",
    "Variable definitions\n",
    "sentiment: Sentiment of tweet\n",
    "\n",
    "message: Tweet body\n",
    "\n",
    "tweetid: Twitter unique id\n",
    "\n",
    "Files available for download\n",
    "train.csv - You will use this data to train your model.\n",
    "\n",
    "test.csv - You will use this data to test your model.\n",
    "\n",
    "SampleSubmission.csv - is an example of what your submission file should look like. The order of the rows does not matter, but the names of the tweetid's must be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#NLP libaries used\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Splitting/ generating matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Various types of imbalance methodes used:\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Models for prediction and additional sources used\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(r'C:\\Users\\Magdalena\\Desktop\\kaggle\\climate-change-edsa2020-21\\train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(r'C:\\Users\\Magdalena\\Desktop\\kaggle\\climate-change-edsa2020-21\\test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Sentiments\n",
    "Its important to see if your data is balance or not. This is to ensure that there is no bais when the data set will be making predictions as standard algorithms will make us of the majortity sentiment and won't take into consideration the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "train_df.groupby('sentiment').message.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment are classify as the following:\n",
    "\n",
    "- 2: News: the tweet links to factual news about climate change\n",
    "- 1: Pro: the tweet supports the belief of man-made climate change\n",
    "- 0: Neutral: the tweet neither supports nor refutes the belief of man-made climate changes\n",
    "- -1: Anti: the tweet doesn't believe in man-made climate change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    8530\n",
       " 2    3640\n",
       " 0    2353\n",
       "-1    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above value data we can see that the data set is *inbalance* and that we should aim to balance the data once we begin with fitting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing/ Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Removing all the noise in the tweets by replacing or removing:\n",
    "\n",
    "- Working with ascii\n",
    "- URLs\n",
    "- Usernames\n",
    "- Hashtags\n",
    "- Punctuation\n",
    "- repeating of RTs\n",
    "\n",
    "After the noise has been remove the following should take place:\n",
    "- Make all tweets lowercase\n",
    "- Remove all stop words from tweet\n",
    "- Stem/ or Lemmatize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign dataframes to new variables (easier to replace a mistake):\n",
    "train = train_df\n",
    "test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcation will pre-process/ clean the tweets accoring to each describtion below:\n",
    "\n",
    "def data_clean(msg):\n",
    "    msg = msg.encode('ascii', 'ignore').decode() #accepts custom handlers for errors in ascii\n",
    "    msg = re.sub(r'http\\S+', 'url', msg) #replace are urls/https with the wording url\n",
    "    #msg = re.sub(r'@[A-Za-z0-9]+','',msg) # remove username\n",
    "    msg = re.sub(r'RT', '', msg) # remove the word RT/ aka Retweet from the message\n",
    "    msg = re.sub('[%s]' % re.escape(string.punctuation), '', msg) #remove all punctuation to see all print (sting.punctuation)\n",
    "    msg = re.sub(r'[0-9]', '', msg) # remove all digits from message\n",
    "    msg = msg.lower() #make whole mage into lower case \n",
    "    msg = re.sub(r'\\s+',' ', msg) # remove white spave from message\n",
    "       \n",
    "    return msg                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply above function on both the test and training set and asssign cleaned data to column tweet:\n",
    "train['tweet'] = train['message'].apply(lambda msg:data_clean(msg))\n",
    "test['tweet'] = test['message'].apply(lambda msg:data_clean(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenise the tweet to ensure that we can peform Lemmatization and also remove stop words from each tweet\n",
    "\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "train['tokens'] = train['tweet'].apply(tokeniser.tokenize)\n",
    "test['tokens'] = test['tweet'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for lemmatize of the words\n",
    "\n",
    "def lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the lemmatizer function to dataframe and genrate a new column\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train['lemma'] = train['tokens'].apply(lemma, args=(lemmatizer, ))\n",
    "test['lemma'] = test['tokens'].apply(lemma, args=(lemmatizer, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for stopwords removal\n",
    "def remove_stop_words(tokens):\n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the stop function to dataframe and genrate a new column\n",
    "\n",
    "train['w_stp'] = train['tokens'].apply(remove_stop_words)\n",
    "test['w_stp'] = test['tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure that data is in a list format for when creating so that it can be readable for the Modeling\n",
    "\n",
    "train['final']=[\" \".join(x) for x in train['w_stp'].values]\n",
    "test['final']=[\" \".join(x) for x in test['w_stp'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>w_stp</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
       "      <td>[it, not, like, we, lack, evidence, of, anthro...</td>\n",
       "      <td>[like, lack, evidence, anthropogenic, global, ...</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "      <td>rawstory researchers say we have three years ...</td>\n",
       "      <td>[rawstory, researchers, say, we, have, three, ...</td>\n",
       "      <td>[rawstory, researcher, say, we, have, three, y...</td>\n",
       "      <td>[rawstory, researchers, say, three, years, act...</td>\n",
       "      <td>rawstory researchers say three years act clima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "      <td>todayinmaker wired was a pivotal year in the w...</td>\n",
       "      <td>[todayinmaker, wired, was, a, pivotal, year, i...</td>\n",
       "      <td>[todayinmaker, wired, wa, a, pivotal, year, in...</td>\n",
       "      <td>[todayinmaker, wired, pivotal, year, war, clim...</td>\n",
       "      <td>todayinmaker wired pivotal year war climate ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "      <td>soynoviodetodas its and a racist sexist clima...</td>\n",
       "      <td>[soynoviodetodas, its, and, a, racist, sexist,...</td>\n",
       "      <td>[soynoviodetodas, it, and, a, racist, sexist, ...</td>\n",
       "      <td>[soynoviodetodas, racist, sexist, climate, cha...</td>\n",
       "      <td>soynoviodetodas racist sexist climate change d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221   \n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562   \n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736   \n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  polyscimajor epa chief doesnt think carbon dio...   \n",
       "1  its not like we lack evidence of anthropogenic...   \n",
       "2   rawstory researchers say we have three years ...   \n",
       "3  todayinmaker wired was a pivotal year in the w...   \n",
       "4   soynoviodetodas its and a racist sexist clima...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [its, not, like, we, lack, evidence, of, anthr...   \n",
       "2  [rawstory, researchers, say, we, have, three, ...   \n",
       "3  [todayinmaker, wired, was, a, pivotal, year, i...   \n",
       "4  [soynoviodetodas, its, and, a, racist, sexist,...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [it, not, like, we, lack, evidence, of, anthro...   \n",
       "2  [rawstory, researcher, say, we, have, three, y...   \n",
       "3  [todayinmaker, wired, wa, a, pivotal, year, in...   \n",
       "4  [soynoviodetodas, it, and, a, racist, sexist, ...   \n",
       "\n",
       "                                               w_stp  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [like, lack, evidence, anthropogenic, global, ...   \n",
       "2  [rawstory, researchers, say, three, years, act...   \n",
       "3  [todayinmaker, wired, pivotal, year, war, clim...   \n",
       "4  [soynoviodetodas, racist, sexist, climate, cha...   \n",
       "\n",
       "                                               final  \n",
       "0  polyscimajor epa chief doesnt think carbon dio...  \n",
       "1    like lack evidence anthropogenic global warming  \n",
       "2  rawstory researchers say three years act clima...  \n",
       "3  todayinmaker wired pivotal year war climate ch...  \n",
       "4  soynoviodetodas racist sexist climate change d...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() #Recalling up headers to ensure the correct data gets select When splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>w_stp</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>europe will now be looking to china to make su...</td>\n",
       "      <td>[europe, will, now, be, looking, to, china, to...</td>\n",
       "      <td>[europe, will, now, be, looking, to, china, to...</td>\n",
       "      <td>[europe, looking, china, make, sure, alone, fi...</td>\n",
       "      <td>europe looking china make sure alone fighting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>combine this with the polling of staffers re c...</td>\n",
       "      <td>[combine, this, with, the, polling, of, staffe...</td>\n",
       "      <td>[combine, this, with, the, polling, of, staffe...</td>\n",
       "      <td>[combine, polling, staffers, climate, change, ...</td>\n",
       "      <td>combine polling staffers climate change womens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "      <td>the scary unimpeachable evidence that climate ...</td>\n",
       "      <td>[the, scary, unimpeachable, evidence, that, cl...</td>\n",
       "      <td>[the, scary, unimpeachable, evidence, that, cl...</td>\n",
       "      <td>[scary, unimpeachable, evidence, climate, chan...</td>\n",
       "      <td>scary unimpeachable evidence climate change al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "      <td>karoli morgfair osborneink dailykos putin got ...</td>\n",
       "      <td>[karoli, morgfair, osborneink, dailykos, putin...</td>\n",
       "      <td>[karoli, morgfair, osborneink, dailykos, putin...</td>\n",
       "      <td>[karoli, morgfair, osborneink, dailykos, putin...</td>\n",
       "      <td>karoli morgfair osborneink dailykos putin got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "      <td>fakewillmoore female orgasms cause global war...</td>\n",
       "      <td>[fakewillmoore, female, orgasms, cause, global...</td>\n",
       "      <td>[fakewillmoore, female, orgasm, cause, global,...</td>\n",
       "      <td>[fakewillmoore, female, orgasms, cause, global...</td>\n",
       "      <td>fakewillmoore female orgasms cause global warm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  Europe will now be looking to China to make su...   169760   \n",
       "1  Combine this with the polling of staffers re c...    35326   \n",
       "2  The scary, unimpeachable evidence that climate...   224985   \n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263   \n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  europe will now be looking to china to make su...   \n",
       "1  combine this with the polling of staffers re c...   \n",
       "2  the scary unimpeachable evidence that climate ...   \n",
       "3  karoli morgfair osborneink dailykos putin got ...   \n",
       "4   fakewillmoore female orgasms cause global war...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [europe, will, now, be, looking, to, china, to...   \n",
       "1  [combine, this, with, the, polling, of, staffe...   \n",
       "2  [the, scary, unimpeachable, evidence, that, cl...   \n",
       "3  [karoli, morgfair, osborneink, dailykos, putin...   \n",
       "4  [fakewillmoore, female, orgasms, cause, global...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [europe, will, now, be, looking, to, china, to...   \n",
       "1  [combine, this, with, the, polling, of, staffe...   \n",
       "2  [the, scary, unimpeachable, evidence, that, cl...   \n",
       "3  [karoli, morgfair, osborneink, dailykos, putin...   \n",
       "4  [fakewillmoore, female, orgasm, cause, global,...   \n",
       "\n",
       "                                               w_stp  \\\n",
       "0  [europe, looking, china, make, sure, alone, fi...   \n",
       "1  [combine, polling, staffers, climate, change, ...   \n",
       "2  [scary, unimpeachable, evidence, climate, chan...   \n",
       "3  [karoli, morgfair, osborneink, dailykos, putin...   \n",
       "4  [fakewillmoore, female, orgasms, cause, global...   \n",
       "\n",
       "                                               final  \n",
       "0  europe looking china make sure alone fighting ...  \n",
       "1  combine polling staffers climate change womens...  \n",
       "2  scary unimpeachable evidence climate change al...  \n",
       "3  karoli morgfair osborneink dailykos putin got ...  \n",
       "4  fakewillmoore female orgasms cause global warm...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head() #Recalling up headers to ensure the correct data gets select When splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into Train X and Y and Test X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data according to what is needed:\n",
    "# y values will be the sentiment that we are using to predict outcome\n",
    "# X values are the actual tweet that's been cleaned up\n",
    "\n",
    "y = train['sentiment']\n",
    "X = train['final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Test set also went through the cleaning process and there fore will assign only the final tweet to new variable\n",
    "testx = test['final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectirorizer with a limitation on min_df for only 4 values\n",
    "tfidf = TfidfVectorizer(min_df=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the tfidf on the Train and Test set to convert all data into matrix\n",
    "X_vec = tfidf.fit_transform(X)\n",
    "test_ve = tfidf.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into train and test sets on the data that's been converted into a matrix:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11864, 13248)\n",
      "(11864,)\n"
     ]
    }
   ],
   "source": [
    "#Print the shape of the sets where (rows, columns) of each one after splitting\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imblance dataset\n",
    "\n",
    "I will make use of 4 different types to work with imblance data sets: \n",
    "- SMOTE\n",
    "- ADASYN\n",
    "- RandomUnderSampler\n",
    "- Tomek Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the  object with the desired sampling strategy.\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "\n",
    "#fit model on train set\n",
    "x_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17248, 4782)\n",
      "(17248,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_smote.shape)\n",
    "print(y_train_smote.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the object to resample the majority class.\n",
    "adasyn = ADASYN(sampling_strategy=\"minority\")\n",
    "\n",
    "# fit the object to the training data.\n",
    "x_train_adasyn, y_train_adasyn = adasyn.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17267, 4782)\n",
      "(17267,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_adasyn.shape)\n",
    "print(y_train_adasyn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the object.\n",
    "under_sampler = RandomUnderSampler()\n",
    "\n",
    "# fit the object to the training data.\n",
    "x_train_under, y_train_under = under_sampler.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3952, 4782)\n",
      "(3952,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_under.shape)\n",
    "print(y_train_under.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TomekLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_links = TomekLinks()\n",
    "\n",
    "# fit the object to the training data.\n",
    "x_train_tl, y_train_tl = tomek_links.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10625, 13248)\n",
      "(10625,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tl.shape)\n",
    "print(y_train_tl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SGD Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "sgd.fit(X_train, y_train)\n",
    "pred_sgd = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6343619963322383"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_sgd, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_smote = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "sgd_smote.fit(x_train_smote, y_train_smote)\n",
    "pred_sgd_smote = sgd_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5947483090822916"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_sgd_smote, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_adsyn = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "sgd_adsyn.fit(x_train_adasyn, y_train_adasyn)\n",
    "pred_sgd_adsyn = sgd_adsyn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575020769688493"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_sgd_adsyn, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome on the SGD Classifier:\n",
    "Submission score of: 0.72498\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SGDC.Classifier with Tomeklinks data sampleing\n",
    "\n",
    "Tomek links: Pairs data points that belong to different classes but with close intances\n",
    "These samples are near the borderline between classes. \n",
    "\n",
    "Using this we would remove majority class of each pair, and increase the space between the two classes. \n",
    "This will the move the data towards a more blancing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_tk = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "sgd_tk.fit(x_train_tl, y_train_tl)\n",
    "pred_sgd_tk = sgd_tk.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6322979935198003"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_sgd_tk, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate submission:\n",
    "1. Apply model on the Test.csv which is now know as test_vec (the data set that's been vectorized)\n",
    "2. \"Concat\" the Tweetid with the sentiment and export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = sgd_tk.predict(test_ve)\n",
    "test['sentiment'] = y_preds\n",
    "test[['tweetid', 'sentiment']].to_csv('SGDClassifier_TL.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 4 dataset were use on the model and the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.553892535018651"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "f1_score(y_test, rfc_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5528241774044149"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "f1_score(y_test, rfc_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.553892535018651"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_s = RandomForestClassifier()\n",
    "rfc_s.fit(x_train_smote, y_train_smote)\n",
    "rfc_pred_s = rfc.predict(X_test)\n",
    "f1_score(y_test, rfc_pred_s, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5908387645405553"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_a = RandomForestClassifier()\n",
    "rfc_a.fit(x_train_adasyn, y_train_adasyn)\n",
    "rfc_pred_a = rfc_a.predict(X_test)\n",
    "f1_score(y_test, rfc_pred_a, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome of the RandomForestClassifier ADASYN sample:\n",
    "Submission Score on kaggle: 0.70021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5922242848150339"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_a = RandomForestClassifier()\n",
    "rfc_a.fit(x_train_adasyn, y_train_adasyn)\n",
    "rfc_pred_a = rfc_a.predict(X_test)\n",
    "f1_score(y_test, rfc_pred_a, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = rfc_a.predict(test_ve)\n",
    "test['sentiment'] = y_preds\n",
    "test[['tweetid', 'sentiment']].to_csv('RFC_ADASYN.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SVM Classifier Kernel: Linear and rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.552650261028376"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svml = SVC(C=20000, gamma= 'auto', kernel='linear')\n",
    "svml.fit(X_train, y_train)\n",
    "predl = svml.predict(X_test)\n",
    "f1_score(y_test, predl, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6176320400179927"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(C=20000, gamma= 'auto', kernel='rbf')\n",
    "svm.fit(X_train, y_train)\n",
    "predl = svm.predict(X_test)\n",
    "f1_score(y_test, predl, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5540851863654243"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_tll = SVC(C=20000, gamma= 'auto', kernel='linear')\n",
    "svm_tll.fit(x_train_tl, y_train_tl)\n",
    "predtl = svm_tll.predict(X_test)\n",
    "f1_score(y_test, predtl, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = svm_tl.predict(test_ve)\n",
    "test['sentiment'] = y_preds\n",
    "test[['tweetid', 'sentiment']].to_csv('SVM_yl_rbf_normal.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission score: 0.68551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5459412669853071"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asvm_l = SVC(C=20000, gamma= 'auto', kernel='linear')\n",
    "asvm_l.fit(x_train_adasyn, y_train_adasyn)\n",
    "apredl = asvm_l.predict(X_test)\n",
    "f1_score(y_test, apredl, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6053126594427288"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asvm = SVC(C=20000, gamma= 'auto', kernel='rbf')\n",
    "asvm.fit(x_train_adasyn, y_train_adasyn)\n",
    "apred = asvm.predict(X_test)\n",
    "f1_score(y_test, apred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6056766645904647"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssvm_l = SVC(C=20000, gamma= 'auto', kernel='rbf')\n",
    "ssvm_l.fit(x_train_smote, y_train_smote)\n",
    "spred_l = ssvm_l.predict(X_test)\n",
    "f1_score(y_test, spred_l, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54477014829483"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssvm = SVC(C=20000, gamma= 'auto', kernel='linear')\n",
    "ssvm.fit(x_train_smote, y_train_smote)\n",
    "spred = ssvm.predict(X_test)\n",
    "f1_score(y_test, spred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5840451306187933"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_tl = SVC(kernel='rbf')\n",
    "svm_tl.fit(x_train_tl, y_train_tl)\n",
    "pred_tl = svm_tl.predict(X_test)\n",
    "f1_score(y_test, pred_tl, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning:\n",
    "- First started with Cs = [0.001, 0.01, 0.1, 1, 10] and gammas = [0.001, 0.01, 0.1, 1]\n",
    "- Played around with the Cs and gammas ended with prefered C: 10 and gamma = 1\n",
    "- Kernel prevernce rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter tuning \n",
    "def param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds, n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "params = param_selection(X_train, y_train,5)\n",
    "print (params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_svm_tl = SVC(kernel='rbf', C=10, gamma = 1)\n",
    "final_svm_tl.fit(x_train_tl, y_train_tl)\n",
    "svc_pred = final_svm_tl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6388085111633679"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, svc_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Outcome of the SVM Classifier, with C=10 and gamma = 1 on Tomek links dataset\n",
    "Submission Score on kaggle: 0.73103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = final_svm_tl.predict(test_ve)\n",
    "test['sentiment'] = y_preds\n",
    "test[['tweetid', 'sentiment']].to_csv('SVM__grid_tl_rbf_normal.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried to make another prediction with gridsearch although the F1- Score is much beter with C as 100\n",
    "#the final score on kaggle only got to 0.7307\n",
    "#parameter tuning \n",
    "#def param_selection(X, y, nfolds):\n",
    "#Cs = [0.1, 1, 10, 100]\n",
    "#gammas = [0.1, 1, 10, 100]\n",
    "#param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "#grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds, n_jobs=-1)\n",
    "#grid_search.fit(X, y)\n",
    "#grid_search.best_params_\n",
    "#return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100, 'gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "#params = sparam_selection(x_train_tl, y_train_tl,5)\n",
    "#print (params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_svm2_tl = SVC(kernel='rbf', C=100, gamma = 1)\n",
    "#final_svm2_tl.fit(x_train_tl, y_train_tl)\n",
    "#svc_pred = final_svm2_tl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6391008116228727"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f1_score(y_test, svc_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = final_svm2_tl.predict(test_ve)\n",
    "test['sentiment'] = y_preds\n",
    "test[['tweetid', 'sentiment']].to_csv('SVM__grid_100_tl_rbf_normal.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
